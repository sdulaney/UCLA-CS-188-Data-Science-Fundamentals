{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to **CS188 - Data Science Fundamentals!** We plan on having you go through some grueling training so you can start crunching data out there... in today's day and age \"data is the new oil\" or perhaps \"snake oil\" nonetheless, there's a lot of it, each with different purity (so pure that perhaps you could feed off it for a life time) or dirty which then at that point you can either decide to dump it or try to weed out something useful (that's where they need you... )\n",
    "\n",
    "\n",
    "In this project you will work through an example project end to end. \n",
    "\n",
    "Here are the main steps:\n",
    "\n",
    "1. Get the data\n",
    "2. Visualize the data for insights\n",
    "3. Preprocess the data for your machine learning algorithm\n",
    "4. Select a model and train\n",
    "5. Does it meet the requirements? Fine tune the model\n",
    "\n",
    "![steps](MLprocess.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best to experiment with real-data as opposed to aritifical datasets. \n",
    "\n",
    "There are many different open datasets depending on the type of problems you might be interested in!\n",
    "\n",
    "Here are a few data repositories you could check out:\n",
    "- [UCI Datasets](http://archive.ics.uci.edu/ml/)\n",
    "- [Kaggle Datasets](kaggle.com)\n",
    "- [AWS Datasets](https://registry.opendata.aws)\n",
    "\n",
    "Below we will run through an California Housing example collected from the 1990's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5) # python>=3.5\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\" # sklearn >= 0.20\n",
    "\n",
    "import numpy as np #numerical package in python\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt #plotting package\n",
    "\n",
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)\n",
    "\n",
    "#matplotlib magic for inline figures\n",
    "%matplotlib inline \n",
    "import matplotlib # plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Where to save the figures\n",
    "ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_name, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    '''\n",
    "        plt.savefig wrapper. refer to \n",
    "        https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.savefig.html\n",
    "    '''\n",
    "    path = os.path.join(IMAGES_PATH, fig_name + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_name)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "DATASET_PATH = os.path.join(\"datasets\", \"housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Data Exploration Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will load the dataset, and visualize different\n",
    "features using different types of plots.\n",
    "\n",
    "Packages we will use:\n",
    "- **[Pandas](https://pandas.pydata.org):** is a fast, flexibile and expressive data structure widely used for tabular and multidimensional datasets.\n",
    "- **[Matplotlib](https://matplotlib.org)**: is a 2d python plotting library which you can use to create quality figures (you can plot almost anything if you're willing to code it out!)\n",
    "    - other plotting libraries:[seaborn](https://seaborn.pydata.org), [ggplot2](https://ggplot2.tidyverse.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data(DATASET_PATH) # we load the pandas dataframe\n",
    "housing.head() # show the first few elements of the dataframe\n",
    "               # typically this is the first thing you do\n",
    "               # to see how the dataframe looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset may have different types of features\n",
    "- real valued\n",
    "- Discrete (integers)\n",
    "- categorical (strings)\n",
    "\n",
    "The two categorical features are essentialy the same as you can always map a categorical string/character to an\n",
    "integer. \n",
    "\n",
    "In the dataset example, all our features are real valued floats, except ocean proximity which is categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see a concise summary of data types, null values, and counts\n",
    "# use the info() method on the dataframe\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can access individual columns similarly\n",
    "# to accessing elements in a python dict\n",
    "housing[\"ocean_proximity\"].head() # added head() to avoid printing many columns.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to access a particular row we can use iloc\n",
    "housing.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one other function that might be useful is\n",
    "# value_counts(), which counts the number of occurences\n",
    "# for categorical features\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The describe function compiles your typical statistics for each\n",
    "# column\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you want to learn about different ways of accessing elements or other functions it's useful to check out the getting started section [here](https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can draw a histogram for each of the dataframes features\n",
    "# using the hist function\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "# save_fig(\"attribute_histogram_plots\")\n",
    "plt.show() # pandas internally uses matplotlib, and to display all the figures\n",
    "           # the show() function must be called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to have a histogram on an individual feature:\n",
    "housing[\"median_income\"].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert a floating point feature to a categorical feature\n",
    "by binning or by defining a set of intervals. \n",
    "\n",
    "For example, to bin the \n",
    "households based on median_income we can use the pd.cut function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign each bin a categorical value [1, 2, 3, 4, 5] in this case.\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing[\"income_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"income_cat\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next let's visualize the household incomes based on latitude & longitude coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'housing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d995ba9b3ceb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## here's a not so interestting way plotting it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhousing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"scatter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"longitude\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latitude\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msave_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bad_visualization_plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'housing' is not defined"
     ]
    }
   ],
   "source": [
    "## here's a not so interestting way plotting it\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\n",
    "save_fig(\"bad_visualization_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can make it look a bit nicer by using the alpha parameter, \n",
    "# it simply plots less dense areas lighter.\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)\n",
    "save_fig(\"better_visualization_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A more interesting plot is to color code (heatmap) the dots\n",
    "# based on income. The code below achieves this\n",
    "\n",
    "# load an image of california\n",
    "images_path = os.path.join('./', \"images\")\n",
    "os.makedirs(images_path, exist_ok=True)\n",
    "filename = \"california.png\"\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "california_img=mpimg.imread(os.path.join(images_path, filename))\n",
    "ax = housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n",
    "                       s=housing['population']/100, label=\"Population\",\n",
    "                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
    "                       colorbar=False, alpha=0.4,\n",
    "                      )\n",
    "# overlay the califronia map on the plotted scatter plot\n",
    "# note: plt.imshow still refers to the most recent figure\n",
    "# that hasn't been plotted yet.\n",
    "plt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5,\n",
    "           cmap=plt.get_cmap(\"jet\"))\n",
    "plt.ylabel(\"Latitude\", fontsize=14)\n",
    "plt.xlabel(\"Longitude\", fontsize=14)\n",
    "\n",
    "# setting up heatmap colors based on median_house_value feature\n",
    "prices = housing[\"median_house_value\"]\n",
    "tick_values = np.linspace(prices.min(), prices.max(), 11)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.set_yticklabels([\"$%dk\"%(round(v/1000)) for v in tick_values], fontsize=14)\n",
    "cb.set_label('Median House Value', fontsize=16)\n",
    "\n",
    "plt.legend(fontsize=16)\n",
    "save_fig(\"california_housing_prices_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not suprisingly, the most expensive houses are concentrated around the San Francisco/Los Angeles areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we have only visualized feature histograms and basic statistics. \n",
    "\n",
    "When developing machine learning models the predictiveness of a feature for a particular target of intrest is what's important.\n",
    "\n",
    "It may be that only a few features are useful for the target at hand, or features may need to be augmented by applying certain transfomrations. \n",
    "\n",
    "None the less we can explore this using correlation matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example if the target is \"median_house_value\", most correlated features can be sorted\n",
    "# which happens to be \"median_income\". This also intuitively makes sense.\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the correlation matrix for different attributes/features can also be plotted\n",
    "# some features may show a positive correlation/negative correlation or\n",
    "# it may turn out to be completely random!\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "save_fig(\"scatter_matrix_plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median income vs median house vlue plot plot 2 in the first row of top figure\n",
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n",
    "             alpha=0.1)\n",
    "plt.axis([0, 16, 0, 550000])\n",
    "save_fig(\"income_vs_house_value_scatterplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting Features\n",
    "New features can be created by combining different columns from our data set.\n",
    "\n",
    "- rooms_per_household = total_rooms / households\n",
    "- bedrooms_per_room = total_bedrooms / total_rooms\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain new correlations\n",
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"rooms_per_household\", y=\"median_house_value\",\n",
    "             alpha=0.2)\n",
    "plt.axis([0, 5, 0, 520000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dastaset for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've visualized the data, and have a certain understanding of how the data looks like. It's time to clean!\n",
    "\n",
    "Most of your time will be spent on this step, although the datasets used in this project are relatively nice and clean... it could get real dirty.\n",
    "\n",
    "After having cleaned your dataset you're aiming for:\n",
    "- train set\n",
    "- test set\n",
    "\n",
    "In some cases you might also have a validation set as well for tuning hyperparameters (don't worry if you're not familiar with this term yet..)\n",
    "\n",
    "In supervised learning setting your train set and test set should contain (**feature**, **target**) tuples. \n",
    " - **feature**: is the input to your model\n",
    " - **target**: is the ground truth label\n",
    "     - when target is categorical the task is a classification task\n",
    "     - when target is floating point the task is a regression task\n",
    "     \n",
    "We will make use of **[scikit-learn](https://scikit-learn.org/stable/)** python package for preprocessing. \n",
    "\n",
    "Scikit learn is pretty well documented and if you get confused at any point simply look up the function/object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# let's first start by creating our train and test sets\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    train_set = housing.loc[train_index]\n",
    "    test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = train_set.drop(\"median_house_value\", axis=1) # drop labels for training set features\n",
    "                                                       # the input to the model should not contain the true label\n",
    "housing_labels = train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing With Incomplete Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have you noticed when looking at the dataframe summary certain rows \n",
    "# contained null values? we can't just leave them as nulls and expect our\n",
    "# model to handle them for us...\n",
    "sample_incomplete_rows = housing[housing.isnull().any(axis=1)].head()\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows.dropna(subset=[\"total_bedrooms\"])    # option 1: simply drop rows that have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_incomplete_rows.drop(\"total_bedrooms\", axis=1)       # option 2: drop the complete feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"total_bedrooms\"].median() \n",
    "sample_incomplete_rows[\"total_bedrooms\"].fillna(median, inplace=True) # option 3: replace na values with median values\n",
    "sample_incomplete_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you think of another plausible imputation for this dataset? (Not graded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell implements the complete pipeline for preparing the data\n",
    "# using sklearns TransformerMixins\n",
    "# Earlier we mentioned different types of features: categorical, and floats.\n",
    "# In the case of floats we might want to convert them to categories.\n",
    "# On the other hand categories in which are not already represented as integers must be mapped to integers before\n",
    "# feeding to the model.\n",
    "\n",
    "# Additionally, categorical values could either be represented as one-hot vectors or simple as normalized/unnormalized integers.\n",
    "# Here we encode them using one hot vectors.\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\") # use median imputation for missing values\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1) # remove the categorical feature\n",
    "# column index\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n",
    "\n",
    "# \n",
    "class AugmentFeatures(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    implements the previous features we had defined\n",
    "    housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "    housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "    housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\n",
    "    '''\n",
    "    def __init__(self, add_bedrooms_per_room = True): \n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing else to do\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = AugmentFeatures(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('attribs_adder', AugmentFeatures()),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "numerical_features = list(housing_num)\n",
    "categorical_features = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numerical_features),\n",
    "        (\"cat\", OneHotEncoder(), categorical_features),\n",
    "    ])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model and train\n",
    "\n",
    "Once we have prepared the dataset it's time to choose a model.\n",
    "\n",
    "As our task is to predict the median_house_value (a floating value), regression is well suited for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# let's try the full preprocessing pipeline on a few training instances\n",
    "data = test_set.iloc[:5]\n",
    "labels = housing_labels.iloc[:5]\n",
    "data_prepared = full_pipeline.transform(data)\n",
    "\n",
    "print(\"Predictions:\", lin_reg.predict(data_prepared))\n",
    "print(\"Actual labels:\", list(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate our model using certain metrics, a fitting metric for regresison is the mean-squared-loss\n",
    "\n",
    "$$L(\\hat{Y}, Y) = \\sum_i^N (\\hat{y_i} - y_i)^2$$\n",
    "\n",
    "where $\\hat{y}$ is the predicted value, and y is the ground truth label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = lin_reg.predict(housing_prepared)\n",
    "mse = mean_squared_error(housing_labels, preds)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Applying the end-end ML steps to a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply what we've learnt to another dataset (airbnb dataset). We will predict airbnb price based on other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [25 pts] Visualizing Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Load the data + statistics\n",
    "\n",
    "- load the dataset\n",
    "- display the first few rows of the data\n",
    "- drop the following columns: name, host_id, host_name, last_review\n",
    "- display a summary of the statistics of the loaded data\n",
    "- plot histograms for 3 features of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Plot total number_of_reviews per neighbourhood_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Plot map of airbnbs throughout New York (if it gets too crowded take a subset of the data, and try to make it look nice if you can :) ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Plot average price of room types who have availability greater than 180 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Plot correlation matrix\n",
    "- which features have positive correlation?\n",
    "- which features have negative correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [25 pts] Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Set aside 20% of the data as test test (80% train, 20% test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Augment the dataframe with two other features which you think would be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] Impute any missing feature with a method of your choice, and briefly discuss why you chose this imputation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 pts] Code complete data pipeline using sklearn mixins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [15 pts] Fit a model of your choice\n",
    "\n",
    "The task is to predict the price, you could refer to the housing example on how to train and evaluate your model using MSE.\n",
    "Provide both test and train set MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
